{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wir trainieren nur bergab? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Das Problem der Regression\n",
    "Bei der Regressionsanalyse muss eine Modellfunktion gefunden werden, die zu einem gegebenen Satz von Datenpunkten N möglichst **genau** passt. Ein häufig verwendetes Maß für die Genauigkeit der Approximation ist die **Methode der kleinsten Quadrate** (engl. least squares approach). Der vertikale Abstand zwischen jedem Datenpunkt $(x_n,y_n)$ und der Ausgabe der Modellfunktion $m(x_n)$ wird durch Subtraktion der y-Werte der Datenpunkte von den vorhergesagten y-Werten der Modellfunktion berechnet (3).\n",
    "\n",
    "\\begin{align}\n",
    "d_n & = m(x_n)-y_n \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;        (3)\n",
    "\\end{align}\n",
    "\n",
    "Diese Abstände werden dann quadriert und aufsummiert. Da wir die Qualität einer Approximation mit anderen Approximationen vergleichen wollen, die möglicherweise eine andere Anzahl von Datenpunkten haben, teilen wir die Summe noch durch die Gesamtzahl der Datenpunkte (**mittlerer quadratischer Fehler**). So erhalten wir unsere **Verlust**-Funktion (4). Unser Ziel ist es, diese Metrik so niedrig wie möglich zu halten, denn je niedriger der Verlust, desto besser die Approximation. Hier haben die Begriffe \"Verlust\" und \"Fehler\" die gleiche Bedeutung. Ein anderer häufig verwendeter Begriff ist \"Kosten\".\n",
    "\n",
    "\\begin{align}\n",
    "Verlust & = \\frac{1}{N} \\sum_{n=0}^{N-1} {[m(x_n)-y_n]}^2 \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; (4)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Wenn wir eine genaue Regression erreicht haben, können wir damit **Vorhersagen** machen. Wir werden unsere Neuronen auf einen gegebenen Satz von Punkten trainieren und sie dann verwenden, um neue Punkte vorherzusagen. Dazu werden wir dem trainierten Neuron neue x-Werte geben, zu denen es die y-Werte vorhersagen soll.\n",
    "\n",
    "\n",
    "<img src=\"images/least_squares_explanation.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Abb. 3 - Visualisierung des Abstands zur Modellfunktion\n",
    "</p>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "class SimpleNeuron:\n",
    "    def __init__(self, plot):\n",
    "        self.plot = plot #I am assigned the following plot\n",
    "        self.plot.register_neuron(self) #hey plot, remember me\n",
    "        \n",
    "    def set_values(self, weight:float, bias:float):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "        self.plot.update() #hey plot, I have changed, redraw my output\n",
    "        \n",
    "    def get_weight(self) -> float:\n",
    "        return self.weight\n",
    "    \n",
    "    def get_bias(self) -> float:\n",
    "        return self.bias\n",
    "\n",
    "    def compute(self, x:float) -> float:\n",
    "        self.activation = self.weight * x + self.bias\n",
    "        return self.activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir werden eine Funktion \"loss\" erstellen, die die Operation in Gleichung (4) durchführt. Als Argumente wird sie dazu ein Neuron-Objekt und einen Satz von Punkten erhalten.\n",
    "- Für jeden Punkt, den wir ihr geben, trennt sie zunächst x- und y-Werte. \n",
    "- Dann übergibt sie dem Neuron einen x-Wert und bittet das Neuron, eine Vorhersage für den y-Wert zu berechnen. (siehe $m(x_n)$) \n",
    "- Dann subtrahiert sie den realen y-Wert von dem vorhergesagten y-Wert, wie in Gleichung (3), und erhält einen Abstand\n",
    "- Anschließend quadriert sie den Abstand und summiert die quadrierten Abstände.  \n",
    "- Im letzten Schritt wird die Summe der quadrierten Abstände durch die Anzahl der verglichenen Punkte dividiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Führen Sie die nachfolgende Zelle aus, um eine Verlustfunktion zu definieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "def loss(neuron:SimpleNeuron, points:dict) -> float:\n",
    "    sum_squared_dist = 0\n",
    "\n",
    "    for point_x, point_y in zip(points[\"x\"], points[\"y\"]):  # zip merges both points[\"x\"] and points[\"y\"]\n",
    "\n",
    "        predicted_point_y = neuron.compute(point_x)\n",
    "        dist = point_y - predicted_point_y\n",
    "        squared_dist = dist ** 2\n",
    "        sum_squared_dist += squared_dist\n",
    "\n",
    "    loss = sum_squared_dist / len(points[\"y\"])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorbereiten eines interaktiven Plots\n",
    "\n",
    "Nachdem wir die notwendigen Bibliotheken importiert haben, werden wir nun eine interaktive Plot-Klasse einrichten. Diese soll die Ausgabe eines Neurons zeichnen, indem sie es auffordert, einen Satz von x-Werten zu berechnen. Das führt zu einem Satz von vorhergesagten y-Werten, die auf einer Ebene gezeichnet werden können. Wenn das Gewicht oder der Bias eines Neurons geändert wird, ruft das Neuron die \"redraw\"-Methode seines Plots auf, um ihn zu aktualisieren. Der Plot kann auch feste Punkte zeichnen. Interaktive Schieberegler werden verwendet, um die Gewichte und den Bias der Neuronenobjekten direkt zu ändern.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Führen Sie die nachfolgenden Zellen aus, um die Bibliotheken zu importieren und einen interaktiven Plot zu definieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "import numpy as np\n",
    "import plotly.offline as plotly\n",
    "import plotly.graph_objs as go\n",
    "from ipywidgets import interact, Layout, HBox, FloatSlider\n",
    "import time\n",
    "import threading\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "# an Interactive Plot monitors the activation of a neuron or a neural network\n",
    "class Interactive2DPlot:\n",
    "    def __init__(self, points:dict, ranges:dict, width:int=800, height:int=400, margin:dict=dict(t=0, l=170), draw_time:float=0.05):\n",
    "        self.idle = True\n",
    "        self.points = points\n",
    "        self.x = np.arange(ranges[\"x\"][0], ranges[\"x\"][1], 0.1)\n",
    "        self.y = np.arange(ranges[\"y\"][0], ranges[\"y\"][1], 0.1)\n",
    "        self.draw_time = draw_time\n",
    "        self.layout = go.Layout(\n",
    "            xaxis=dict(title=\"Input: x\", range=ranges[\"x\"], fixedrange=True),\n",
    "            yaxis=dict(title=\"Output: y\", range=ranges[\"y\"], fixedrange=True),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            showlegend=False,\n",
    "            autosize=False,\n",
    "            margin=margin,\n",
    "        )\n",
    "        self.trace = go.Scatter(x=self.x, y=self.y)\n",
    "        self.plot_points = go.Scatter(x=points[\"x\"], y=points[\"y\"], mode=\"markers\")\n",
    "        self.data = [self.trace, self.plot_points]\n",
    "        self.plot = go.FigureWidget(self.data, self.layout)\n",
    "        # self.plot = plotly.iplot(self.data, self.layout,config={\"displayModeBar\": False})\n",
    "\n",
    "    def register_neuron(self, neuron:SimpleNeuron):\n",
    "        self.neuron = neuron\n",
    "\n",
    "    def redraw(self):\n",
    "        self.idle = False\n",
    "        time.sleep(self.draw_time)\n",
    "        self.plot.data[0].y = self.neuron.compute(self.x)\n",
    "        self.idle = True\n",
    "\n",
    "    def update(self):\n",
    "        print(\"Loss: {:0.2f}\".format(loss(self.neuron, self.points)))\n",
    "        if self.idle:\n",
    "            thread = threading.Thread(target=self.redraw)\n",
    "            thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 4.2.1:</b> Das Neuron trainieren\n",
    "<ul>\n",
    "<li>Sie erhalten einen Satz mit 3 Punkten und ein Neuron, um eine Kurvenanpassung durchzuführen. Führen Sie die folgende Zelle aus.\n",
    "<li> <b>Verändern Sie das Gewicht und den Bias des Neurons mit den Schiebereglern, um den Verlust zu minimieren.</b>\n",
    "    <li><b>Tipp:</b> Sie können die Schieberegler auch mit den Pfeiltasten auf Ihrer Tastatur verändern, nachdem Sie auf den Schieberegler geklickt haben.\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5e427f5d874f6a890d71b47deae00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='weight', layout=Layout(width='90%'), max=3.0, min=-3…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08e85132fc642eeb5d8dfb3c889ef64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': '83ef51bb-855e-4f16-ac06-bf4d9da6cd09',\n",
       "              'x': {'bdata': ('AAAAAAAAEMAzMzMzMzMPwGZmZmZmZg' ... 'mZmQ1AdmZmZmZmDkBEMzMzMzMPQA=='),\n",
       "                    'dtype': 'f8'},\n",
       "              'y': {'bdata': ('AAAAAAAAEMAzMzMzMzMPwGZmZmZmZg' ... 'mZmQ1AdmZmZmZmDkBEMzMzMzMPQA=='),\n",
       "                    'dtype': 'f8'}},\n",
       "             {'mode': 'markers',\n",
       "              'type': 'scatter',\n",
       "              'uid': '24ab5a17-e5da-4fbd-942c-46823dc40b32',\n",
       "              'x': [1, 2, 3],\n",
       "              'y': [1.5, 0.7, 1.2]}],\n",
       "    'layout': {'autosize': False,\n",
       "               'height': 400,\n",
       "               'margin': {'l': 170, 't': 0},\n",
       "               'showlegend': False,\n",
       "               'template': '...',\n",
       "               'width': 800,\n",
       "               'xaxis': {'fixedrange': True, 'range': [-4, 4], 'title': {'text': 'Input: x'}},\n",
       "               'yaxis': {'fixedrange': True, 'range': [-4, 4], 'title': {'text': 'Output: y'}}}\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do not change\n",
    "points_linreg = dict(x=[1, 2, 3], y=[1.5, 0.7, 1.2])\n",
    "ranges_linreg = dict(x=[-4, 4], y=[-4, 4])\n",
    "\n",
    "linreg_plot = Interactive2DPlot(points_linreg, ranges_linreg)\n",
    "simple_neuron = SimpleNeuron(linreg_plot)\n",
    "\n",
    "slider_layout = Layout(width=\"90%\")\n",
    "\n",
    "interact(\n",
    "    simple_neuron.set_values, \n",
    "    weight=FloatSlider(min=-3, max=3, step=0.1, value = 0, layout=slider_layout),\n",
    "    bias=FloatSlider(min=-3, max=3, step=0.1, value = 0, layout=slider_layout)\n",
    ")\n",
    "\n",
    "linreg_plot.plot\n",
    "# plt.plot(1,1,'x')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 4.2.2:</b> Was ist die optimale Kombination aus Gewicht und Bias? \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b> Ein Gewicht von -0.1 und ein Bias von 1.4 mit einem Loss 0.1</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorbereiten des 3D-Plots\n",
    "Wir sehen, dass die Suche nach dem geringsten Verlust ein **Parameteroptimierungsproblem** ist. Bisher können wir das Problem manuell lösen. Wollen wir aber neuronale Netze zur Lösung komplexerer Probleme verwenden, müssen wir einen Weg finden, diesen Prozess zu automatisieren.\n",
    "\n",
    "Die Verlustfunktion wird sowohl mit dem festgelegten Gewicht als auch mit dem festgelegten Bias geändert. Diese Beziehung kann dreidimensional visualisiert werden, was uns weitere Einblicke geben kann, um einen Algorithmus zu konstruieren, der das Optimierungsproblem löst. \n",
    "In dieser 3D-Ansicht werden logarithmische Skalen verwendet, um die Topographie hervorzuheben. Wir werden eine neue Funktion definieren, um den logarithmischen Verlust für einen Satz von Punkten zu berechnen.\n",
    "\n",
    "Der Plot wird wie folgt definiert:\n",
    "- Die **X-Achse** stellt die Gewichte dar. \n",
    "- Die **Y-Achse** stellt den Bias dar.\n",
    "- Die **Z-Achse** (Höhe) stellt den entsprechenden Verlustwert bei einer gegebenen Gewicht/Bias-Konfiguration dar. Zur besseren Verständlichkeit wird der Logarithmus des MSE-Verlusts angezeigt.\n",
    "- Die **schwarze Kugel** stellt die aktuelle Gewicht/Bias-Konfiguration. Seine Höhe zeigt den Verlust dieser Konfiguration an."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Führen Sie die folgenden Zellen aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "def log_mse(neuron:SimpleNeuron, points:dict) -> float:\n",
    "    least_squares_loss = loss(neuron, points)\n",
    "    return np.log10(least_squares_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "class Interactive3DPlot:\n",
    "    def __init__(self, points:dict, ranges:dict, width:int=600, height:int=600, draw_time:int=0.1):\n",
    "        self.idle = True\n",
    "        self.points = points\n",
    "        self.draw_time = draw_time\n",
    "        self.threading = threading\n",
    "\n",
    "        self.range_weights = np.arange(  # Array with all possible weight values in the given range\n",
    "            ranges[\"x\"][0], ranges[\"x\"][1], 0.1\n",
    "        )\n",
    "        self.range_biases = np.arange(  # Array with all possible bias values in the given range\n",
    "            ranges[\"y\"][0], ranges[\"y\"][1], 0.1\n",
    "        )\n",
    "        self.range_biases_t = self.range_biases[:, np.newaxis]  # Bias array transposed\n",
    "        self.range_losses = []  # initialize z axis for 3D surface\n",
    "\n",
    "        self.ball = go.Scatter3d(  # initialize ball\n",
    "            x=[], y=[], z=[], hoverinfo=\"none\", mode=\"markers\", marker=dict(size=12, color=\"black\")\n",
    "        )\n",
    "\n",
    "        self.layout = go.Layout(\n",
    "            width=width,\n",
    "            height=height,\n",
    "            showlegend=False,\n",
    "            autosize=False,\n",
    "            margin=dict(t=0, l=0),\n",
    "            scene=dict(\n",
    "                xaxis=dict(title=\"Weight\", range=ranges[\"x\"], autorange=False, showticklabels=True),\n",
    "                yaxis=dict(title=\"Bias\", range=ranges[\"y\"], autorange=False, showticklabels=True),\n",
    "                zaxis=dict(title=\"Loss: log(MSE)\", range=ranges[\"z\"], autorange=True, showticklabels=False),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.data = [\n",
    "            go.Surface(\n",
    "                z=self.range_losses,\n",
    "                x=self.range_weights,\n",
    "                y=self.range_biases,\n",
    "                colorscale=\"Viridis\",\n",
    "                opacity=0.9,\n",
    "                showscale=False,\n",
    "                hoverinfo=\"none\",\n",
    "            ),\n",
    "            self.ball,\n",
    "        ]\n",
    "\n",
    "        self.plot = go.FigureWidget(self.data, self.layout)\n",
    "\n",
    "    def register_neuron(self, neuron:SimpleNeuron):\n",
    "        self.neuron = neuron\n",
    "        self.calc_surface()\n",
    "\n",
    "        # height of 3d surface represents loss of weight/bias combination\n",
    "        # In the 2D plot, x is an array from e.g. -4 to +4. But the weights and biases only have a single value\n",
    "        # Here x will be the points to do regression and to calculate the loss on. \n",
    "        # The surface is spanned by the arrays of weight and bias.\n",
    "        \n",
    "    def calc_surface(self):  \n",
    "                \n",
    "        self.neuron.weight = (  #instead of 1 weight and 1 bias, let Neuron have an array of all weights and biases\n",
    "            self.range_weights\n",
    "        )\n",
    "        self.neuron.bias = self.range_biases_t\n",
    "        self.range_losses = log_mse(  # result: matrix of losses of all weight/bias combinations in the given range\n",
    "            self.neuron, self.points\n",
    "        )\n",
    "        self.plot.data[0].z = self.range_losses\n",
    "\n",
    "    def update(self):\n",
    "        if self.idle:\n",
    "            thread = threading.Thread(target=self.redraw)\n",
    "            thread.start()\n",
    "\n",
    "    def redraw(self):  # when updating, only the ball is redrawn\n",
    "        self.idle = False\n",
    "        time.sleep(self.draw_time)\n",
    "        self.ball.x = [self.neuron.weight]\n",
    "        self.ball.y = [self.neuron.bias]\n",
    "        self.ball.z = [log_mse(self.neuron, self.points)]\n",
    "        self.plot.data[1].x = self.ball.x\n",
    "        self.plot.data[1].y = self.ball.y\n",
    "        self.plot.data[1].z = self.ball.z\n",
    "        self.idle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "class DualPlot:\n",
    "    def __init__(self, points:dict, ranges_3d:dict, ranges_2d:dict):\n",
    "        self.plot_3d = Interactive3DPlot(points, ranges_3d)\n",
    "        self.plot_2d = Interactive2DPlot(points, ranges_2d, width=400, height=500, margin=dict(t=200, l=30))\n",
    "\n",
    "    def register_neuron(self, neuron:SimpleNeuron):\n",
    "        self.plot_3d.register_neuron(neuron)\n",
    "        self.plot_2d.register_neuron(neuron)\n",
    "\n",
    "    def update(self):\n",
    "        self.plot_3d.update()\n",
    "        self.plot_2d.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 4.2.3:</b> Das Neuron trainieren\n",
    "<ul>\n",
    "<li> Sie erhalten den gleiche Satz von 3 Datenpunkten und erneut ein Neuron, um eine Kurvenanpassung durchzuführen. Führen Sie die folgende Zelle aus.\n",
    "<li> <b>Verändern Sie das Gewicht und den Bias des Neurons mit den Schiebereglern, um den Verlust zu minimieren.</b>\n",
    "<li> <b>Beobachten Sie alle Änderungen.</b>\n",
    "    </li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Hinweis:</b> Sie können den 3D-Plot drehen, indem Sie auf ihn klicken und den Cursor bewegen. Dabei müssen Sie mit dem Cursor innerhalb des Widgets bleiben. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985be1f4fab34558a81fd0e377554ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='weight', layout=Layout(width='90%'), max=2.0, min=-2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357cf77895e04cf3b4ec53b0a9093268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FigureWidget({\n",
       "    'data': [{'colorscale': [[0.0, '#440154'], [0.1111111111111111, '#482878'],\n",
       "…"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do not change\n",
    "ranges_3d = dict(x=[-2.5, 2.5], y=[-2.5, 2.5], z=[-1, 2.5])  # set up ranges for the 3d plot\n",
    "plot_task2 = DualPlot(points_linreg, ranges_3d, ranges_linreg)  # create a DualPlot object to mange plotting on two plots\n",
    "neuron_task2 = SimpleNeuron(plot_task2)  # create a new neuron for this task\n",
    "\n",
    "interact(\n",
    "    neuron_task2.set_values,\n",
    "    weight=FloatSlider(min=-2, max=2, step=0.2, layout=slider_layout),\n",
    "    bias=FloatSlider(min=-2, max=2, step=0.2, layout=slider_layout),\n",
    ")\n",
    "\n",
    "HBox((plot_task2.plot_3d.plot, plot_task2.plot_2d.plot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 4.2.4:</b> Wo liegt im Allgemeinen die optimale Kombination aus Gewicht und Bias im 3D-Plot?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b> Sie liegt im Minimum der Verlustfunktion (hier aus allen Bias - Gewicht - Kombinationen berechnet)</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 4.2.5:</b> Wie steil ist das Tal an der Stelle der optimalen Gewichts- und Bias Kombination?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b> Gewicht: (0.11-0.42)/0.2=-1.5, Bias = (0.11-0.21)/0.2=-0.5 </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Aktivierungsfunktionen\n",
    "Bis jetzt ist unser Neuronenmodell, das aus Gewichten und Bias besteht, nur in der Lage, lineare Funktionen zu imitieren. Folglich können wir damit nur lineare Regression durchführen. Aktivierungsfunktionen erweitern unsere Möglichkeiten, indem sie dem Neuron eine zusätzliche Nichtlinearität hinzufügen. Mit ihnen können wir komplexere Funktionen modellieren. Die heutzutage am häufigsten verwendete Aktivierungsfunktion ist die Rectified Linear Unit, auch **ReLU** genannt. Sie gibt nur den Eingabewert aus, solange er größer als 0 ist. Ist er kleiner als 0, gibt sie 0 aus. Wir können diese Funktion bequem beschreiben, indem wir das Maximum des Eingabewertes und 0 nehmen. Der größere Wert von beiden wird als Ausgabe gewählt (5).\n",
    "\n",
    "\\begin{align}\n",
    "f_{relu}(x) & = max(0,x) \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; (5)\n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Führen Sie die folgende Zelle aus, um die ReLU Funktion zu definieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "def relu(input_val:float) -> float:\n",
    "    return np.where(input_val > 0, input_val, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können ein Neuron mit einer ReLU-Aktivierungsfunktion folgendermaßen zeichnen:\n",
    "\n",
    "<img src=\"images/single_neuron_relu.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Abb. 5 - Neuron mit ReLU-Aktivierungsfunktion visualisiert\n",
    "</p>\n",
    "Lassen Sie uns eine neue Klasse erstellen, um dieses Neuron in Python zu implementieren. Wir werden alle Eigenschaften eines Neurons von SimpleNeuron erben.\n",
    "Wir ändern nur die Ausgabe, indem wir sie zunächst durch unsere ReLU-Funktion führen:\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 4.2.6:</b> Implementieren Sie ein komplettes künstlichen Neurons mit ReLU-Aktivierungsfunktion\n",
    "<ul>\n",
    "<li> Vervollständigen Sie den folgenden Code eines künstliches Neuron, indem Sie die ReLU-Funktion von oben verwenden, um seine Aktivierung zu berechnen (wie in Abbildung 5). </li>\n",
    "<li>Schauen Sie sich die <a href=\"#simple_neuron\">einfache Neuronenklasse</a> an und schreiben Sie eine ähnliche Berechnungsfunktion</li>\n",
    "<li>Sie brauchen die relu-Funktion nicht neu zu implementieren und sollten nicht mehr als 1 Zeile hinzufügen müssen. </li>\n",
    "\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluNeuron(SimpleNeuron): #inherit from SimpleNeuron class\n",
    "    \n",
    "    def compute(self, inputs:list) -> float:\n",
    "        # STUDENT CODE HERE\n",
    "        self.activation = relu(self.weight * inputs + self.bias)\n",
    "        # STUDENT CODE until HERE\n",
    "        return self.activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Aufgabe: Nichtlineare Klimaregelung\n",
    "\n",
    "Sie finden sich als Ingenieur bei der Firma \"ClimaTronics\" wieder. Ihre Firma möchte KI-Technologie implementieren, um ihr neues Klimasystem \"Perfect Climate 9000\" zu regeln. Obwohl das Problem leicht mit konventioneller Programmierung gelöst werden kann, möchte die Geschäftsleitung, dass Sie KI implementieren, um Investoren zu gewinnen. Sie müssen die folgenden Anforderungen erfüllen, die im Datenblattauszug visualisiert sind:\n",
    "\n",
    "\n",
    "`Bei Temperaturen unter 25°C soll die Klimasteuerung ausgeschaltet bleiben. Bei einer Temperatur von 30°C soll sie 10% ihrer Kühlleistung erreichen. Zwischen 30°C und 40°C soll die Kühlleistung quadratisch mit der Temperatur ansteigen. Die Kühlleistung soll bei 40°C ihr Maximum erreichen.`\n",
    "\n",
    "<img src=\"images/datasheet.png\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Führen Sie die nachfolgende Zelle aus, um ein interaktives Diagramm anzuzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46e1699c9414f78bdd6239578fa77bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='weight', layout=Layout(width='90%'), max=10.0, min=-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6dedc5fa034d43a96ea889c190e633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': '2a03c97d-79d0-4416-8e33-420a5c372cdd',\n",
       "              'x': {'bdata': ('AAAAAAAAEMAzMzMzMzMPwGZmZmZmZg' ... 'mZmVlGQGxmZmZmZkZAOTMzMzNzRkA='),\n",
       "                    'dtype': 'f8'},\n",
       "              'y': {'bdata': ('AAAAAAAAEMAzMzMzMzMPwGZmZmZmZg' ... 'zMzCxaQDozMzMzM1pAoJmZmZk5WkA='),\n",
       "                    'dtype': 'f8'}},\n",
       "             {'mode': 'markers',\n",
       "              'type': 'scatter',\n",
       "              'uid': '671a0e45-e4f8-469a-ab09-046cd5b2cc45',\n",
       "              'x': [25.0, 27.5, 30.0, 32.5, 35, 37.5, 40.0],\n",
       "              'y': [0.0, 2.0, 10.0, 23.7, 43, 68.7, 100.0]}],\n",
       "    'layout': {'autosize': False,\n",
       "               'height': 400,\n",
       "               'margin': {'l': 170, 't': 0},\n",
       "               'showlegend': False,\n",
       "               'template': '...',\n",
       "               'width': 800,\n",
       "               'xaxis': {'fixedrange': True, 'range': [-4, 45], 'title': {'text': 'Input: x'}},\n",
       "               'yaxis': {'fixedrange': True, 'range': [-4, 105], 'title': {'text': 'Output: y'}}}\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do not change\n",
    "points_climate = dict(x=[25.0, 27.5, 30.0, 32.5, 35, 37.5, 40.0], y=[0.0, 2.0, 10.0, 23.7, 43, 68.7, 100.0])\n",
    "\n",
    "ranges_climate = dict(x=[-4, 45], y=[-4, 105])\n",
    "climate_plot = Interactive2DPlot(points_climate, ranges_climate)\n",
    "our_relu_neuron = ReluNeuron(climate_plot)\n",
    "\n",
    "interact(\n",
    "    our_relu_neuron.set_values,\n",
    "    weight=FloatSlider(min=-10, max=10, step=0.1, value=0, layout=slider_layout),\n",
    "    bias=FloatSlider(min=-200.0, max=200.0, step=1, value=0, layout=slider_layout),\n",
    ")\n",
    "\n",
    "climate_plot.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 4.2.7:</b> Wie wirkt sich eine Änderung des Gewichts auf die Ausgabefunktion aus, wenn der Bias auf 0,00 gesetzt wird? \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b> Die Steigung der Funktion ändert sich.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 4.2.8:</b> Wie wirkt sich die Änderung des Bias auf die Ausgangsfunktion aus? \n",
    "</div>\n",
    "\n",
    "<div class=\"alert block alert-success\">\n",
    "<b>Ihre Antwort:</b> Der Input Wert ab dem eine Aktivierung (!=0) stattfindet ändert sich.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 4.2.9:</b> Bei welcher Temperatur beginnt die Klimatisierung, wenn sie das Gewicht auf 1,00 und den Bias auf -10, setzen? \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b> Bei einer Temperatur von 10°C</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 4.2.10:</b> Bei welcher Temperatur beginnt die Klimatisierung, wenn Sie das Gewicht auf 1,00 und den Bias auf -20 setzen?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b>Bei einer Temperatur von 20°C</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 4.2.11:</b> Bei welcher Temperatur beginnt die Klimatisierung, wenn Sie das Gewicht auf 2,00 und des Bias auf -20 setzen?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b> Sie beginnt immer noch bei 20°C</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 4.2.12:</b> Was ist die beste Gewicht/Bias-Konfiguration, die Sie finden konnten?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert block alert-success\">\n",
    "<b>Ihre Antwort:</b>Bias=-200, Gewicht=7.1</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schlussfolgerung\n",
    "Mit nur einem Neuron können wir den Einfluss von Gewicht und Bias leicht verstehen und nachvollziehen.\n",
    "Aber unsere Ein-Neuron-Approximation reicht nicht aus, um die benötigte quadratische Beziehung genau zu approximieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Neuronale Netze\n",
    "\n",
    "Die Approximation kann durch die Verwendung mehrerer Neuronen verbessert werden. Anstatt nur ein Neuron für unsere Approximation zu verwenden, konstruieren wir ein neuronales Netzwerk. Wir werden zwei ReLU-Neuronen und ein Ausgangsneuron verwenden, das ebenfalls Gewichte hat. Nun können wir entscheiden, wie wir das Ergebnis der beiden ReLU-Neuronen in der Mitte gewichten wollen.\n",
    "\n",
    "### Versteckte Schichten (engl. Hidden Layers)\n",
    "In dem nachfolgenden neuronalen Netzwerk stellen die beiden Neuronen in der Mitte eine **versteckte Schicht** dar.\n",
    "\n",
    "In der letzten Aufgabe hatten das Gewicht und der Bias einen leicht nachvollziehbaren Einfluss auf die Ausgabe.\n",
    "Durch das Hinzufügen weiterer Neuronen wird die Beziehung zwischen den einzelnen Gewichten und Bias und der Ausgabe jedoch weniger nachvollziehbar.\n",
    "Wir erhalten die Gewichte und Bias, indem wir sie einfach solange anpassen, bis das Ergebnis stimmt. Bei diesem Vorgang verlieren wir schnell den Überblick darüber, was wir eigentlich genau berechnen. Daher ist es schwierig, ein einzelnes Neuron aus diesem Netz zu isolieren und seine Verantwortung im System zu beschreiben.\n",
    "\n",
    "Der Eingabewert wird mit den ersten Gewichten multipliziert und nach dem Hinzufügen der Bias auf eine Aktivierungsfunktion gegeben. Anschließend wird dieser Ausgang mit den zweiten Gewichten multipliziert. Versteckte Schichten können mehrfach hintereinander gestapelt werden. Dies gibt Raum für mehrere Berechnungsschritte und ermöglicht die Approximation komplexerer Funktionen.\n",
    "\n",
    "Neuronale Netze, die mindestens eine versteckte Schicht verwenden, haben eine interessante Eigenschaft: Sie können zur Approximation einer beliebigen stetigen Funktion verwendet werden. _(Siehe \"Weiterführende Literatur\")_\n",
    "\n",
    "<img src=\"images/hidden_layer.png\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir werden eine Klasse für neuronale Netze erstellen. Das Netzwerk wird vier Gewichte und zwei Bias haben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Hinweis:</b> Der Einfachheit halber und zur Wiederverwendbarkeit des Codes werden wir neuronale Netze so behandeln, wie wir einzelne Neuronen in den bisherigen Beispielen behandelt haben. Denken Sie daran, dass ein künstliches Neuron nur eine mathematische Funktion ist. Ein ganzes neuronales Netzwerk kann auch vollständig durch eine einzelne Funktion beschrieben werden, wie es auch bei der Berechnung der Aktivierung geschieht. Die Neuronen müssen nicht die konkrete Form von einzelnen Datenobjekten annehmen.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Führen Sie die folgende Zelle aus, um ein neuronales Netzwerk zu definieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, plot):\n",
    "        self.plot = plot #I am assigned the following plot\n",
    "        self.plot.register_neuron(self) #hey plot, remember me\n",
    "        \n",
    "    def set_config(self, w_i1:float, w_o1:float, b1:float, w_i2:float, w_o2:float, b2:float):\n",
    "        self.w_i1 = w_i1\n",
    "        self.w_o1 = w_o1\n",
    "        self.b1 = b1\n",
    "        self.w_i2 = w_i2\n",
    "        self.w_o2 = w_o2\n",
    "        self.b2 = b2\n",
    "        self.show_config()\n",
    "        self.plot.update()  # please redraw my output\n",
    "\n",
    "    def show_config(self):\n",
    "        print(\"w_i1:\", self.w_i1, \"\\t| \", \"w_o1:\", self.w_o1,\"\\n\")\n",
    "        print(\"b1:\", self.b1, \"\\t| \", \"w_i2:\", self.w_i2,\"\\n\")\n",
    "        print(\"w_o2:\", self.w_o2, \"\\t| \", \"b2:\", self.b2,\"\\n\")\n",
    "\n",
    "    def compute(self, x:float)->float:\n",
    "        self.prediction = (relu(self.w_i1 * x + self.b1) * self.w_o1\n",
    "                         + relu(self.w_i2 * x + self.b2) * self.w_o2)\n",
    "        return self.prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Aufgabe: Nichtlineare Klimaregelung mit einem neuronalen Netzwerk\n",
    "\n",
    "Führen Sie die folgende Zelle aus und passen Sie Gewichte und Bias an, um eine bessere Annäherung an die gewünschte Kurve als in der vorherigen Aufgabe zu erreichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a358bd831b5a4f128920893de861ea5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='w_i1', layout=Layout(width='90%'), max=10.0, min=-10…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30fd617d87347d4b7ddbb6f0b013e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': 'ed20daae-cbc6-4c05-9bba-26cca9688abf',\n",
       "              'x': {'bdata': ('AAAAAAAAEMAzMzMzMzMPwGZmZmZmZg' ... 'mZmVlGQGxmZmZmZkZAOTMzMzNzRkA='),\n",
       "                    'dtype': 'f8'},\n",
       "              'y': {'bdata': ('AAAAAAAAEMAzMzMzMzMPwGZmZmZmZg' ... 'zMzCxaQDozMzMzM1pAoJmZmZk5WkA='),\n",
       "                    'dtype': 'f8'}},\n",
       "             {'mode': 'markers',\n",
       "              'type': 'scatter',\n",
       "              'uid': '5a52c1b2-90b5-45a4-87fa-1f7b11b34e42',\n",
       "              'x': [25.0, 27.5, 30.0, 32.5, 35, 37.5, 40.0],\n",
       "              'y': [0.0, 2.0, 10.0, 23.7, 43, 68.7, 100.0]}],\n",
       "    'layout': {'autosize': False,\n",
       "               'height': 400,\n",
       "               'margin': {'l': 170, 't': 0},\n",
       "               'showlegend': False,\n",
       "               'template': '...',\n",
       "               'width': 800,\n",
       "               'xaxis': {'fixedrange': True, 'range': [-4, 45], 'title': {'text': 'Input: x'}},\n",
       "               'yaxis': {'fixedrange': True, 'range': [-4, 105], 'title': {'text': 'Output: y'}}}\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do not change\n",
    "climate_plot_adv = Interactive2DPlot(points_climate, ranges_climate)\n",
    "our_neural_net = NeuralNetwork(climate_plot_adv)\n",
    "\n",
    "interact(\n",
    "    our_neural_net.set_config,\n",
    "    w_i1=FloatSlider(min=-10, max=10, step=0.1, layout=slider_layout),\n",
    "    w_o1=FloatSlider(min=-10, max=10, step=0.1,  layout=slider_layout),\n",
    "    b1=FloatSlider(min=-200.0, max=200.0, step=1,  layout=slider_layout),\n",
    "    w_i2=FloatSlider(min=-10, max=10, step=0.1, layout=slider_layout),\n",
    "    w_o2=FloatSlider(min=-10, max=10, step=0.1,  layout=slider_layout),\n",
    "    b2=FloatSlider(min=-200.0, max=200.0, step=1,layout=slider_layout),\n",
    ")\n",
    "climate_plot_adv.plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 4.2.13:</b> Was ist die beste Konfiguration, die Sie finden konnten? (Kopie von oberhalb des Plots)\n",
    "</div>\n",
    "\n",
    "<div class=\"alert block alert-success\">\n",
    "<b>Ihre Antwort:</b></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schlussfolgerung\n",
    "Wir stellen fest, dass die quadratische Beziehung durch die Verwendung zusätzlicher Gewichte und Bias besser angenähert werden kann. Mit zwei ReLU Neuronen können wir eine Funktion mit zwei Knicken erstellen.\n",
    "Die Komplexität der Suche nach den optimalen Gewichten/Biases nimmt jedoch mit jeder Variablen drastisch zu. Je leistungsfähiger unsere neuronalen Netze sein sollen, desto schwieriger wird die Optimierung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Backpropagation\n",
    "\n",
    "Die Lösung für unser Optimierungsproblem lautet Backpropagation. Mit ihr können wir den Prozess der Anpassung von Gewichten und Bias automatisieren. In diesem Beispiel werden wir zu den Grundlagen zurückkehren und ein einfaches Neuron ohne Aktivierungsfunktion betrachten. Backpropagation funktioniert unter Hinzunahme der partiellen Ableitungen der Verlustfunktion in Bezug auf jedes Gewicht und jeden Bias im Netz. Diese können mit Hilfe der Kettenregel der Infinitesimalrechnung berechnet werden. Die Ausgabe des Netzwerks $\\hat{y} = \\hat{f}(x,\\theta )$\n",
    "(wenn $\\hat{y}$ den vom neuronalen Netz vorhergesagten y-Wert bezeichnet) wird in der Forwardpropagation unter Anwendung der gegebenen Rechenregeln (Multiplizieren mit Gewichten, Summieren mit Bias und Aktivierungsfunktion, bis Sie den Ausgang erreichen) berechnet. Der Verlust wird dann aus dem vorhergesagten und dem Ground-Truth-Wert (tatsächlichem Wert) mit der Verlustfunktion berechnet. Mit diesem Verlust können Sie einfach die partiellen Ableitungen in der so genannten Backpropagation berechnen. Siehe zum Beispiel: [BackpropagationExample](https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html) \n",
    "\n",
    "An jedem Punkt zeigen der Bias- und der Gewichtsgradient in die Richtung des höheren Verlustes. Die Größe des Gradienten repräsentiert den Betrag der Verlustzunahme. \n",
    "\n",
    "Angenommen, wir würden den Verlust in Abb. 6 __maximieren__: Alles, was wir tun müssen, ist, den partiellen Ableitungen zu folgen, indem wir sie zu unserem aktuellen Gewicht/Bias-Punkt addieren. In diesem Beispiel bedeutet das, dass wir das Gewicht stark verringern (siehe die Achsen in Abb. 6) und den Bias um einen geringen Betrag verringern (da seine partielle Ableitung eine geringere Größe hat).\n",
    "\n",
    "Da wir aber mit dem Verlust nach unten gehen wollen, _subtrahieren_ wir den Gradienten von unserem aktuellen Punkt. Dadurch nähern wir uns dem Minimum entgegen. Im nächsten Schritt sind wir so weiter unten und nahe dem Tiefpunkt. Da wir dann aber immer noch nicht nah genug am Tiefpunkt sind, wiederholen wir diese Schritte einfach, bis wir das Minimum erreicht haben.\n",
    "\n",
    "Das Gute an neuronalen Netzwerken ist, dass wir den Gradienten **analytisch** für alle möglichen Datenpunkte bestimmen können. Wir müssen ihn nicht durch numerische Methoden schätzen, wie z. B. durch die Berechnung des Verlusts zweier Gewicht/Bias-Kombinationen dividiert durch den \"Schrittabstand\" (\"Euler-Methode\"). Dieses Vorwissen über den Gradienten macht die Backpropagation vergleichsweise schnell. Leider können wir aber nicht analytisch die Gewicht/Bias-Kombination bestimmen, die die Verlustfunktion auf ihr Minimum bringt. Wir müssen sie immer noch iterativ über viele Schritte finden.\n",
    "\n",
    "Jeder Schritt, den wir machen, wird eine **Epoche** genannt. (In diesem Fall sind _Trainingsschritte_ und _Epochen_ gleichwertig). Da es schwierig ist, festzustellen, ob das Minimum erreicht ist, geben wir die Anzahl der Epochen vor unserem Abstieg an und lassen das Programm dann einfach laufen.\n",
    "\n",
    "Ist die Größe der Gradienten zu groß, werden wir nie ein Minimum erreichen. Das liegt daran, dass unser Algorithmus die Kugel (also den aktuellen Punkt) bei jedem Schritt zu stark bewegen will. Er wird um das Minimum oszillieren, aber nie dort ankommen. Im Extremfall kann die Bewegung sogar bis in die Unendlichkeit oszillieren. Um die Kontrolle über die Größe der Bewegung zu haben, wird der Gradient mit einem Faktor multipliziert, der **Lernrate** (engl. learning rate) genannt wird (auch \"Schrittgröße\" genannt im Gradientenabstieg). Indem wir die Lernrate auf einen optimalen Wert einstellen, können wir Oszillationen verhindern. Ist die Lernrate allerdings zu klein, wird das Netzwerk sehr lange brauchen, um zu \"lernen\", da sich dann die Gewichte und Bias nur sehr langsam ändern.\n",
    "\n",
    "Die Anzahl der Epochen sowie die Lernrate sind so genannte **Hyperparameter**. Sie beeinflussen den Trainingsprozess, sind aber nicht Teil des Netzwerks selbst.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/backprop.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Abb. 6 - Partielle Ableitungen der Verlustfunktion\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation-Plot vorbereiten\n",
    "Wir werden ein neues 3D-Diagramm erstellen, das die vergangenen Gewichts-/Bias-/Verlustwerte darstellt, während wir versuchen, den Verlust Schritt für Schritt zu optimieren. Die schwarze Kugel hinterlässt eine Spur ihrer vergangenen Werte. Führen Sie die nachfolgende Zelle aus, um das Plotten der Backpropagation-Schritte zu aktivieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "plot_backprop = DualPlot(points_linreg, ranges_3d, ranges_linreg)\n",
    "trace_to_plot = go.Scatter3d(x=[], y=[], z=[], hoverinfo=\"none\", mode=\"lines\", line=dict(width=10, color=\"grey\"))\n",
    "\n",
    "plot_backprop.plot_3d.data.append(trace_to_plot)  # Expand 3D Plot to also plot traces\n",
    "plot_backprop.plot_3d.plot = go.FigureWidget(plot_backprop.plot_3d.data, plot_backprop.plot_3d.layout)\n",
    "plot_backprop.plot_3d.draw_time = 0\n",
    "\n",
    "\n",
    "def redraw_with_traces(plot_to_update:DualPlot, neuron:SimpleNeuron, trace_list:dict, points:dict):  # executed every update step\n",
    "    plot_to_update.plot_3d.plot.data[2].x = trace_list[\"x\"]\n",
    "    plot_to_update.plot_3d.plot.data[2].y = trace_list[\"y\"]\n",
    "    plot_to_update.plot_3d.plot.data[2].z = trace_list[\"z\"]\n",
    "    plot_to_update.plot_3d.plot.data[1].x = [neuron.weight]\n",
    "    plot_to_update.plot_3d.plot.data[1].y = [neuron.bias]\n",
    "    plot_to_update.plot_3d.plot.data[1].z = [log_mse(neuron, points)]\n",
    "    plot_to_update.update()\n",
    "\n",
    "\n",
    "def add_traces(neuron:SimpleNeuron, points:dict, trace_list:dict):  # executed every epoch\n",
    "    trace_list[\"x\"].extend([neuron.weight])\n",
    "    trace_list[\"y\"].extend([neuron.bias])\n",
    "    trace_list[\"z\"].extend([log_mse(neuron, points)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## DIY Backpropagation\n",
    "\n",
    "Um Backpropagation durchzuführen, müssen Sie zunächst die partiellen Ableitungen der Verlustfunktion des \"einfachen Neurons\" in Abhängigkeit von Gewicht und Bias bestimmen. Danach müssen Sie herausfinden, wie Sie die Gewichte und Bias richtig an den auf die Lernrate skalierten Gradienten anpassen.\n",
    "Am Ende dieser Übung können Sie Ihre Ergebnisse durch Training verifizieren. Wenn Sie die erwartete Leistung (Benchmark) erreichen, ist Ihr Algorithmus korrekt.\n",
    "\n",
    "Der Algorithmus arbeitet mit einem Dictionary von Punkten mit der Form von: [points_linreg](#points_linreg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 4.2.14:</b> Bestimmen Sie die Steigung <b>analytisch!!</b>\n",
    "<ul>\n",
    "<li> <b>Vervollständigen Sie die folgende Funktion selbst.</b>\n",
    "<li>Es gibt mehrere Lösungen, Ihr Algorithmus kann das Gewicht und den Bias in die richtige Richtung anpassen, obwohl die Berechnung des Gradienten falsch ist.\n",
    "<li> <b>Benchmark:</b> Wenn Sie nach 100 Epochen und mit einer Lernrate von 0,03 einen Verlust von 0,22 erreichen können, ist Ihre Lösung richtig\n",
    "    </li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_neuron_loss_gradient(neuron:SimpleNeuron, points:dict)->dict:\n",
    "\n",
    "    gradient_sum = dict(weight=0, bias=0) # contains the sum of the weight and bias gradient\n",
    "    for point_x, point_y in zip(points[\"x\"], points[\"y\"]):  # for each point\n",
    "            # Hint: point_x and point_y are the current point values\n",
    "        ypred= neuron.compute(point_x)\n",
    "            # STUDENT CODE HERE\n",
    "        if point_y<0:\n",
    "            gradient_sum[\"weight\"] +=0\n",
    "            gradient_sum[\"bias\"] +=0\n",
    "        if point_y>=0:\n",
    "            gradient_sum[\"weight\"] += 2*(ypred-point_y)*point_x\n",
    "            gradient_sum[\"bias\"] +=2*(ypred-point_y)*1\n",
    "        # STUDENT CODE until HERE\n",
    "\n",
    "    gradient = dict(weight=gradient_sum[\"weight\"] / len(points[\"x\"]), bias=gradient_sum[\"bias\"] / len(points[\"x\"]))\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 4.2.15:</b> Das Neuron anpassen\n",
    "<ul>\n",
    "\n",
    "<li>Nachdem Sie den Gradienten ermittelt haben, müssen Sie das Gewicht und den Bias des Neurons basierend auf den partiellen Ableitungen und der Lernrate anpassen. Sie sollten Ihre Ergebnisse überprüfen, indem Sie das Netz unten trainieren.\n",
    "<li> <b>Vervollständigen Sie die nachfolgende Funktion selbst.</b>\n",
    "    </li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_neuron(neuron:SimpleNeuron, gradient:dict, learning_rate:float):\n",
    "    # STUDENT CODE HERE\n",
    "    neuron.weight=neuron.weight-learning_rate*gradient[\"weight\"]\n",
    "    neuron.bias=neuron.bias-learning_rate*gradient[\"bias\"]\n",
    "    # STUDENT CODE until HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainingsprozess definieren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "def train(neuron:SimpleNeuron, points:dict, epochs:int, learning_rate:float, redraw_step:int, trace_list:dict):\n",
    "    redraw_with_traces(neuron.plot, neuron, trace_list, points)\n",
    "    for i in range(1, epochs + 1):  # first Epoch is Epoch no.1\n",
    "        add_traces(neuron, points, trace_list)\n",
    "        gradient = simple_neuron_loss_gradient(neuron, points)\n",
    "        adjust_neuron(neuron, gradient, learning_rate)\n",
    "\n",
    "        if i % redraw_step == 0:\n",
    "            print(\"Epoch:{} \\t\".format(i), end=\"\")\n",
    "            redraw_with_traces(neuron.plot, neuron_backprop, trace_list, points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 4.2.16:</b> Hyperparameter wählen und trainieren\n",
    "<ul>\n",
    "\n",
    "<li>Wählen Sie eine optimale Lernrate und Anzahl der Epochen, indem Sie verschiedene Werte einstellen und die beiden nachfolgenden Zellen ausführen.</li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2fb1d99ec7344d8b0b5be81b8a6ff59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FigureWidget({\n",
       "    'data': [{'colorscale': [[0.0, '#440154'], [0.1111111111111111, '#482878'],\n",
       "…"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.15 #keep this for benchmarking, change to play around\n",
    "epochs = 100 # keep this for benchmarking, change to play around\n",
    "redraw_step = 10 # update plot every n'th epoch. too slow? set this to a higher value (e.g. 100)\n",
    "\n",
    "# these values are taken as parameters by the train function below\n",
    "\n",
    "neuron_backprop = SimpleNeuron(plot_backprop)\n",
    "HBox((plot_backprop.plot_3d.plot, plot_backprop.plot_2d.plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 18.45\n",
      "Loss: 18.45\n",
      "Epoch:10 \tLoss: 0.35\n",
      "Epoch:20 \tLoss: 0.22\n",
      "Epoch:30 \tLoss: 0.15\n",
      "Epoch:40 \tLoss: 0.12\n",
      "Epoch:50 \tLoss: 0.11\n",
      "Epoch:60 \tLoss: 0.10\n",
      "Epoch:70 \tLoss: 0.10\n",
      "Epoch:80 \tLoss: 0.10\n",
      "Epoch:90 \tLoss: 0.09\n",
      "Epoch:100 \tLoss: 0.09\n"
     ]
    }
   ],
   "source": [
    "#run this cell to test algorithm\n",
    "\n",
    "np.random.seed(4) # keep this for benchmarking, remove to play around # TODO: Use np.RandomState !!!!!\n",
    "\n",
    "neuron_backprop.set_values(  # set weight and bias randomly\n",
    "    (5 * np.random.random() - 2.5), (5 * np.random.random() - 2.5)\n",
    ")\n",
    "trace_list1 = dict(x=[], y=[], z=[])\n",
    "\n",
    "train(neuron_backprop, points_linreg, epochs, learning_rate, redraw_step, trace_list1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark:** Wenn Sie nach 100 Epochen und einer Lernrate von 0,03 einen Verlust von 0,22 erreichen können, ist Ihre Lösung korrekt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beantworten Sie diese Fragen erst, wenn Ihr Algorithmus den Benchmark erreicht hat**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 4.2.17:</b> Was passiert, wenn Sie die Lernrate auf 0,18 setzen? Erklären Sie dieses Verhalten.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b> Die Lernrate ist zu groß und die Lösung oszilliert</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 4.2.18:</b> Welche Lernrate führt zu dem geringsten Verlust nach 100 Epochen mit lr=0,03? \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b>l=0.1</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine/Deep Learning Notation\n",
    "\n",
    "Wir haben bereits die Lernrate, die Hyperparameter sowie die Epoche kennengelernt. Nun sollen einige weitere Begriffe eingeführt werden.\n",
    "\n",
    "Betrachten Sie einen Trainingssatz, den Sie an Ihr neuronales Netzwerk geben und dessen Gewichte Sie mit Hilfe von Backpropagation anpassen wollen. Wenn Sie Gewichte und Bias mit jeden Datenpunkt, der in einem Vorwärts- und Rückwärtsdurchlauf verwendet wurde, aktualisieren, spricht man von **Stochastischem Gradientenabstieg** (engl. Stochastic Gradient Descent) oder Online-Lernen. Wie Sie sich vielleicht schon denken könen, können Sie auch die Fehler mehrer Datenpunkte zu Gruppen einer definierten **Stapelgröße** (engl. batch size) zusammen addieren. Auf diese Weise können Sie mehrere Rückwärtsdurchläufe mit diesen größeren Teilmengen von Trainingsdaten durchführen. Dieses Verfahren wird **Batch Gradient Descent** genannt. Bei einer korrekten Berechnung würden Sie den gesamten Satz von Trainingsdatenpunkten für Ihre Vorwärtsdurchläufe verwenden und Fehler speichern, um ein Update zu berechnen (= regulärer Gradient Descent). Mit zunehmender Anzahl an Datenpunkten ist dies aber oft nicht mehr praktikabel, sodass hier Batch Gradient Descent und Stochastic Gradient Descent hilfreich sein können. Allerdings zeigen beide Verfahren während des Trainings unterschiedliche Auswirkungen auf das Modell.\n",
    "\n",
    "Egal, welche Variante des Gradientenabstiegs Sie verwenden: Eine **Epoche** ist dann abgelaufen, wenn Ihre Trainingsdaten einmal vollständig zum Aktualisieren der Gewichte (in Teilmengen oder als Ganzes) verwendet wurden.\n",
    "\n",
    "Wie Sie bereits im Abschnitt zur Performannz Evaluierung gelernt haben, kann ein Modell in Abhängigkeit von der Komplexität des Problems/Modells und der Menge der verfügbaren Daten over- oder underfitten. Overfitting kann in neuronalen Netzen mit **Regularisierung** behoben werden. In der kommenden Einheit zu Convolutional Neural Networks werden Sie einige dieser Techniken unter Zuhilfenahme der Deeplearning-Bibliothek Keras anwenden müssen. Die gängigsten Methoden sind neben der Reduzierung der Modellkomplexität oder der Vergrößerung des Datensatzes die **L1/L2-Regularisierung** oder **Dropout**.\n",
    "\n",
    "Wenn Sie Ihr Modell nicht richtig trainieren können, könnte dies daran liegen, dass Ihr Modell mit **Explodierenden oder verschwindenden Gradienten** (engl. Exploding or Vanishing Gradients) zu kämpfen hat. Betrachten Sie ein sehr tiefes Modell, das mehrere versteckte Schichten enthält. Durch Verwenden des Verlustes am Ausgang sollen die Parameter des gesamten Netzwerks aktualisiert werden. Da Werte $>1$, die rekursiv multipliziert werden, zu größeren Updates führen, kann das Verhalten während des Trainings instabil werden. Das Training wird die Gewichte in früheren Schichten nicht aktualisieren, wenn partielle Ableitungen $<1$ sind und über viele Schichten multipliziert werden (Vanishing Gradient). Um den letzten Fall zu verhindern, der beim regulären Training und beim Verwenden von Sigmoid-Funktionen häufiger auftritt, wird die ReLU-Aktivierungsfunktion heutzutage standardmäßig in den versteckten Schichten verwendet. Desweiteren können auch fortgeschrittenere Methoden verwendet werden, wie z. B. **Residual/Skip-Verbindungen**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Weiterführende Lektüre: Neuronale Netze sind universelle Funktionsapproximatoren\n",
    "\n",
    "Mathematisch kann bewiesen werden, dass neuronale Netze jede kontinuierliche Funktion approximieren können, solange sie mindestens eine versteckte Schicht aufweisen, nichtlineare Aktivierungsfunktionen verwenden und eine ausreichende (aber endliche) Zahl von Neuronen der versteckten Schicht verwenden. \n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/089360809190009T?via%3Dihub\n",
    "\n",
    "Kurt Hornik,\\\n",
    "Approximation capabilities of multilayer feedforward networks,\\\n",
    "Neural Networks,\\\n",
    "Band 4, Ausgabe 2,\\\n",
    "1991,\\\n",
    "Seiten 251-257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AMALEA_3_13 (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
