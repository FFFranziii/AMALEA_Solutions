{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Künstlicher Ghost-Writer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text/Liedtext/Märchen-Generator mit LSTM\n",
    "\n",
    "In diesem Notebook werden Sie lernen, wie Sie mithilfe von Long Short-Term Memory-Netzwerken Texte generieren können. Die Art des Textes, den das Modell generiert, hängt von den Daten ab, mit denen Sie das Netzwerk trainieren. Wenn Sie als Trainingstext einen Liedtext Ihres Lieblingssängers verwenden, wird das LSTM-Modell lernen, Lieder zu schreiben, die diesem ähnlich sind. \n",
    "\n",
    "In diesem Notebook werden wir den Text der Grimms Märchen zum Trainieren verwenden. Der Code basiert auf dem Artikel von Jason Brownlee [How to Develop a Word-Level Neural Language Model and Use it to Generate Text](https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/). \n",
    "\n",
    "Dazu werden wir wie bereits zuvor mit der Datenvorbereitung beginnen, anschließend das LSTM-Modell definieren und es abschließend trainieren und testen.\n",
    "\n",
    "<img src=\"images/grimms_maerchen_cover.jpg\" alt=\"drawing\" style=\"width:200px;\"/>\n",
    "<p style=\"text-align: center;\">\n",
    "    Abb. 1 - Illustriertes Titelblatt des ersten Bandes der zweiten Auflage von 1819\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenvorbereitung\n",
    "\n",
    "In diesem Abschnitt des Notebooks wird die Textdatei in den Speicher geladen, bereinigt (Löschen von Satzzeichen und nicht-alphabetischen Begriffen sowie Umwandlung aller Großbuchstaben in Kleinbuchstaben), um den Lernprozess für das LSTM-Netzwerk zu erleichtern. Die Wörter werden anschließend tokenisiert (Umwandlung von Text in Zahlen), da das Modell am Ende mit Zahlen und nicht mit Zeichenketten arbeitet. Die Textzeilen werden außerdem in Sequenzen mit einer festen Länge zusammengefasst, da das Netzwerk über eine feste Anzahl von Eingangsneuronen verfügt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE GOLDEN BIRD\n",
      "\n",
      "\n",
      "A certain king had a beautiful garden, and in the garden stood a tree\n",
      "which bore golden apples. These apples were always counted, and about\n",
      "the time when they began to grow ripe it was found that every night one\n",
      "of them was gone. The king became very angry at this, and ordered the\n",
      "gardener to keep watch all night under the tree. The gardener set his\n",
      "eldest son to watch; but about twelve oâ€™clock he fell asleep, and in\n",
      "the morning another of the apples was missing. Then the sec\n",
      "\n",
      "\n",
      "['the', 'golden', 'bird', 'a', 'certain', 'king', 'had', 'a', 'beautiful', 'garden', 'and', 'in', 'the', 'garden', 'stood', 'a', 'tree', 'which', 'bore', 'golden', 'apples', 'these', 'apples', 'were', 'always', 'counted', 'and', 'about', 'the', 'time', 'when', 'they', 'began', 'to', 'grow', 'ripe', 'it', 'was', 'found', 'that', 'every', 'night', 'one', 'of', 'them', 'was', 'gone', 'the', 'king', 'became', 'very', 'angry', 'at', 'this', 'and', 'ordered', 'the', 'gardener', 'to', 'keep', 'watch', 'all', 'night', 'under', 'the', 'tree', 'the', 'gardener', 'set', 'his', 'eldest', 'son', 'to', 'watch', 'but', 'about', 'twelve', 'he', 'fell', 'asleep', 'and', 'in', 'the', 'morning', 'another', 'of', 'the', 'apples', 'was', 'missing', 'then', 'the', 'second', 'son', 'was', 'ordered', 'to', 'watch', 'and', 'at']\n",
      "\n",
      "\n",
      "Total Tokens: 95824\n",
      "Unique Tokens: 4698\n",
      "Total Sequences: 95811\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "import string\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename:str)->str:\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc:str)->list:\n",
    "    # replace '--' with a space ' '\n",
    "    doc = doc.replace('--', ' ')\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines:list, filename:str):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "# load document\n",
    "in_filename = 'data/grimms_en.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:500])\n",
    "print(\"\\n\")\n",
    "\n",
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:100])\n",
    "print(\"\\n\")\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    "\n",
    "# organize into sequences of tokens\n",
    "length = 12 + 1      # +1 refers to the to-be-forecast word\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# save sequences to file\n",
    "out_filename = 'grimms_sequences_len12.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 6.4.1:</b> Die Textzeilen werden in Sequenzen mit einer festen Länge zusammengefasst, da das Netzwerk über eine feste Anzahl von Eingangsneuronen verfügt. Welche Länge für die Eingangssequenzen wurde in diesem Code verwendet? \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b>12</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 6.4.2:</b> Warum ist die Anzahl an „Total Tokens“ größer als an „Unique Tokens“? \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b> Die Tokens also die einzelnen Wörter wiederholen sich im Text.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strukturdefinition und Modellierung von LSTM-Netzen\n",
    "\n",
    "Im folgenden wird die Struktur des LSTM-Netzes definiert und das Training durchgeführt. Das Modell wird dabei so aufgebaut, dass es basierend auf einer gegebenen Sequenz ein Wort nach dem anderen vorhersagen kann. Wenn die Sequenz zum Beispiel \"Ich komme aus Spanien, meine Muttersprache ist\" lautet, sollte das LSTM-Modell \"Spanisch\" vorhersagen (natürlich nur, wenn es die Abhängigkeiten zwischen Spanien und Spanisch aus dem Trainingsdatensatz lernen konnte). \n",
    "\n",
    "Um das LSTM-Modell zu definieren, verwenden wir die Keras Bibliothek als Wrapper von Tensorflow. Wenn Sie die GPU nutzen wollen, müssen Sie in der conda-Umgebung tensorflow-gpu installieren. Die Tensorflow Imports bleiben dabei aber unverändert (so wie unten gezeigt). \n",
    "\n",
    "Bei der Definition des LSTM-Netzwerks wird ein [Embedding Layer](https://www.tensorflow.org/text/guide/word_embeddings) hinzugefügt. Dieser wird für Vektorrepräsentationen von Wörtern verwendet, auch bekannt als \"word embeddings\", wie im Artikel [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) von Mikolov et al. beschrieben. Wenn Sie eine sehr gute Erklärung für die Verwendung von Embeddings für die Vektorisierung von Wörtern suchen, empfehlen wir Ihnen, den Artikel [Vector Representations of Words](https://www.tensorflow.org/tutorials/representation/word2vec) von der TensorFlow-Website zu lesen. \n",
    "\n",
    "Spielen Sie gerne mit den Hyperparametern des Modells (Anzahl der Schichten, Neuronen, Optimierer, Batchsize, Epochen, usw.), um bessere Ergebnisse zu erhalten. Sie können auch mit einer Hyperparameter-Optimierungsbibliothek experimentieren, anstatt die Werte von Hand zu ändern. Dazu können Sie die Bibliothek [GpyOpt](https://github.com/SheffieldML/GPyOpt) für Bayes'sche Optimierung oder [HyperOpt](http://hyperopt.github.io/hyperopt/), die Randomsearch und Tree of Parzen Estimators Algorithmen bereitstellt, verwenden. Um mehr über Hyperparameter-Optimierungsmethoden zu erfahren, lesen Sie diese Artikel:\n",
    "- [Hyperparameter-Optimierung in Machine Learning-Modellen](https://www.datacamp.com/community/tutorials/parameter-optimization-machine-learning-models) von Sayak Paul.\n",
    "- [Automated Machine Learning Hyperparameter Tuning in Python](https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a) von Will Koehrsen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries upload\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Protobuf gencode version\")\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "#from keras.optimizers import Adam\n",
    "#from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Führen Sie die nachfolgende Zelle aus, um den Tokenizer zu fitten und die Sequezen zu laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename:str):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load\n",
    "in_filename = 'grimms_sequences_len12.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 6.4.3:</b> Teilen Sie die oben geladenen Eingangs- und Ausgangsdaten in einen Trainings- und Testdatensatz auf (80% zu 20%, zufällig aufgeteilt). Verwenden Sie dafür die Funktion train_test_split() von sklearn und legen Sie die so erhaltenen beiden Splits in den Variablen <code>X_train, X_test, y_train, y_test</code> ab.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "# STUDENT CODE HERE\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2)\n",
    "# STUDENT CODE until HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 6.4.4:</b> Wie werden die „Word Embeddings“ für ein LSTM implementiert?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b>Die Wörter werden als Vektor beschrieben. Eine Vektorkomponente steht für das Wort selber, die andere Komponente beschreibt die Verwandtschaft/Nähe zu anderen Wörtern. </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 6.4.5:</b> Fügen Sie dem im Nachfolgenden definierten Sequential Modell zwei versteckte Schichten hinzu. Die erste soll eine LSTM-Schicht mit 512 Zellen und die zweite eine Dense-Schicht mit 512 Neuronen sein. Verwenden Sie für den Dense-Layer die ReLU-Aktivierungsfunktion.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anderlfr\\gitlab_projects\\AMALEA\\AMALEA_3_13\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/9\n",
      "\u001b[1m1198/1198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 52ms/step - accuracy: 0.0981 - loss: 5.7693 - val_accuracy: 0.1320 - val_loss: 5.3325\n",
      "Epoch 2/9\n",
      "\u001b[1m1198/1198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 60ms/step - accuracy: 0.1463 - loss: 5.0104 - val_accuracy: 0.1524 - val_loss: 5.0744\n",
      "Epoch 3/9\n",
      "\u001b[1m1198/1198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 68ms/step - accuracy: 0.1716 - loss: 4.6136 - val_accuracy: 0.1621 - val_loss: 4.9875\n",
      "Epoch 4/9\n",
      "\u001b[1m1198/1198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 76ms/step - accuracy: 0.2012 - loss: 4.2460 - val_accuracy: 0.1716 - val_loss: 4.9822\n",
      "Epoch 5/9\n",
      "\u001b[1m1198/1198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 75ms/step - accuracy: 0.2352 - loss: 3.8504 - val_accuracy: 0.1749 - val_loss: 5.0601\n",
      "Epoch 6/9\n",
      "\u001b[1m1198/1198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 81ms/step - accuracy: 0.2843 - loss: 3.3969 - val_accuracy: 0.1714 - val_loss: 5.3429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 150, input_length=seq_length))\n",
    "\n",
    "# STUDENT CODE HERE\n",
    "model.add(LSTM(units=512))\n",
    "model.add(Dense(512))\n",
    "# STUDENT CODE until HERE\n",
    "\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "# compile model\n",
    "adam_opt = Adam(clipvalue=1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam_opt, metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=9,validation_data=(X_test,y_test),callbacks=EarlyStopping(monitor='val_loss', patience=2))\n",
    "\n",
    "# save the model to file\n",
    "model.save('model_grimms_150epochs.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer_grimms_150epochs.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 6.4.6:</b> Warum wird die Softmax-Aktivierungsfunktion in der letzten Schicht des Sequential-Modells (die Dense-Layer) verwendet? Also <code>model.add(Dense(vocab_size, activation='softmax'))</code>?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b> Damit für das nächste Wort eine Wahrscheinlichkeit angegeben werden kann. </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelltest\n",
    "\n",
    "Nach der Modellerstellung können wir das LSTM-Netzwerk testen, indem wir neue Grimms Märchen Texte generieren. Dazu wird eine Sequenz in das bereits trainierte Modell eingefügt, als Seed. Das Modell wird das wahrscheinlichste nächste Wort für die Sequenz generieren. Das vorhergesagte Wort wird dabei an den ursprünglichen Satz angehängt und erneut als Modelleingabe verwendet, um ein weiteres Wort vorherzusagen. Dieser Vorgang wird iterativ durchgeführt, bis *n_Wörter* generiert sind. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not even enough of it and he used to look towards the table\n",
      "\n",
      "of the house and when the princess saw that she was to see the bearer and said will not go to the princess and give me a good counsel to eat and said the man is the true princess to be godmother the fourth had to be found who had\n"
     ]
    }
   ],
   "source": [
    "#  code for generating text from the learned-language model is listed below\n",
    "from random import randint\n",
    "from pickle import load\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename:str):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        prob_predictions = model.predict(encoded, verbose=0)[0]\n",
    "        yhat = np.argmax(model.predict(encoded, verbose=0), axis=-1)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "# load cleaned text sequences\n",
    "in_filename = 'grimms_sequences_len12.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model = load_model('model_grimms_150epochs.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer_grimms_150epochs.pkl', 'rb'))\n",
    "\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Sie vielleicht bereits durch die Textgenerierung mit unterschiedlichen Seed Texten bemerkt haben, wiederholt sich der durch das LSTM Modell generierte Text häufig (Endlosschleife). Das liegt daran, dass wir in der `generate_seq()` Funktion mit `yhat = np.argmax(model.predict(encoded, verbose=0), axis=-1)` immer nur das Wort mit der höchsten Vorhersage verwenden. Dieses Problem können wir umgehen, indem wir mit der Numpy `random.choice` Funktion zusätzlich ein bisschen Zufall in die Auswahl des nächsten Wortes hinzufügen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Aufgabe 6.4.7:</b> Verwenden Sie im Folgenden nun die Numpy <code>random.choice</code> Funktion bei der Auswahl des nächsten Wortes, wobei die vorhergesagte Wahrscheinlichkeit jedes Wortes als Wahrscheinlichkeitsverteilung der random-Funktion verwendet werden soll. Legen Sie, wie schon in der <code>generate_seq()</code> Funktion, dieses Ergebnis (das ist der Index des ausgewählten Worts) in der Variablen <code>yhat</code> ab.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not even enough of it and he used to look towards the table\n",
      "\n",
      "as well at the thought from this giants this beautiful flowering shrubs lest he should tongue than i should be knowall prince to worse if you had certainly bury her in the hand and sent me now to my beloved little child and when me you do not best when\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import choice\n",
    "# generate a sequence from a language model using ranom.choice\n",
    "def generate_seq_random(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        prob_predictions = model.predict(encoded, verbose=0)[0]\n",
    "        \n",
    "        # apply random choice\n",
    "        # STUDENT CODE HERE\n",
    "        yhat=choice(range(len(prob_predictions)),p=prob_predictions)\n",
    "        # STUDENT CODE until HERE\n",
    "        \n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "# print seed text\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq_random(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AMALEA_3_13 (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
